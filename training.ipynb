{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.utils.visualize_util import plot\n",
    "#from nltk.corpus import stopwords # Import the stop word list #nltk package for nlp pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import raw input data into the Dataframe \"df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Current working directory is: ', '/Users/Armin/Documents/Deep_Learning/spam_detection')\n",
      "('List of file in directory:\\n', ['.DS_Store', '.git', '.gitignore', '.ipynb_checkpoints', 'bag_of_words_features.pkl', 'data', 'model.png', 'model_loaded.png', 'my_model_architecture.json', 'my_model_weights.h5', 'Untitled.ipynb', 'Untitled1.ipynb'])\n"
     ]
    }
   ],
   "source": [
    "print('Current working directory is: ', os.getcwd())\n",
    "print('List of file in directory:\\n', os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Input_text = pd.read_csv('./data/smsspamcollection.csv', sep='~~', engine='python', header=None)\n",
    "df = pd.DataFrame(Input_text)\n",
    "df.columns = ['Flag','Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unblock this blok to import NLTK for further NLP pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "#print stopwords.words(\"english\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Input data size is:', 5568)\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "input_data_size = Input_text[\"Text\"].size\n",
    "print (\"Input data size is:\", input_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words( raw_text ):\n",
    "    # Function to convert a raw text to a string of words\n",
    "    # The input is a single string , and \n",
    "    # the output is a single string\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_text, \"lxml\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    #letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = review_text.lower().split()                            \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    #stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    #meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the clean input text\n",
    "clean_training_set = []\n",
    "num_inputs = Input_text[\"Text\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 1000 of 5568\n",
      "\n",
      "Message 2000 of 5568\n",
      "\n",
      "Message 3000 of 5568\n",
      "\n",
      "Message 4000 of 5568\n",
      "\n",
      "Message 5000 of 5568\n",
      "\n",
      "The of training set is 5568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in xrange( 0, input_data_size ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean texts\n",
    "    # Following lines give the status of text cleaning\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print \"Message %d of %d\\n\" % ( i+1, num_inputs )  # Monitoring progress     \n",
    "    clean_training_set.append( review_to_words( Input_text[\"Text\"][i] ) )\n",
    "print \"The of training set is %d\\n\" % (len(clean_training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "Done!! Created the bag of words.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Creating the bag of words...\\n\"\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 10000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_training_set)\n",
    "# Save vectorizer for later prediction\n",
    "pickle.dump(vectorizer.vocabulary_,open(\"bag_of_words_features.pkl\",\"wb\"))\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "print \"Done!! Created the bag of words.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!! Created Flags.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 2) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_labels = label_vectorizer.fit_transform(Input_text[\"Flag\"])\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_labels = train_data_labels.toarray()\n",
    "print \"Done!! Created Flags.\\n\"\n",
    "lab = ['spam']\n",
    "lab2 = label_vectorizer.fit_transform(lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup the training and test inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test X is of shape: (1000, 8703)\n",
      "The test Y is of shape: (1000, 2)\n",
      "The Submission set is of shape: (568, 8703)\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "m = 4000 # m determines the number of the data points to be used for training the model\n",
    "Y_train = np.asarray(train_data_labels[0:m])\n",
    "X_train = np.asarray(train_data_features[0:m])\n",
    "Y_test = np.asarray(train_data_labels[m:m+1000])\n",
    "X_test = np.asarray(train_data_features[m:m+1000])\n",
    "X_submission = np.asarray(train_data_features[m+1000:])\n",
    "# convert class vectors to binary class matrices\n",
    "#Y_train = np_utils.to_categorical(Y_train, 2)\n",
    "#Y_test = np_utils.to_categorical(Y_test, 2)\n",
    "\n",
    "print \"The test X is of shape:\",X_test.shape\n",
    "print \"The test Y is of shape:\",Y_test.shape\n",
    "print \"The Submission set is of shape:\",X_submission.shape\n",
    "print X_submission.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup the model global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 8703\n",
    "maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 100\n",
    "num_filters = 32\n",
    "# side length of maxpooling square\n",
    "num_pool = 2\n",
    "# side length of convolution square\n",
    "num_conv = 3\n",
    "filter_length = 3\n",
    "# size of the embedding layer\n",
    "embedding_dims = 100\n",
    "#number of output neurons for the first Dense layer\n",
    "hidden_dims1 = 250\n",
    "#number of output neurons for the second Dense layer\n",
    "hidden_dims2 = 100\n",
    "#this is the length to which each sentence is padded\n",
    "paddedlength = 100\n",
    "# \n",
    "output_size = num_filters * (((paddedlength - filter_length) / 1) + 1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Design model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, batch_input_shape=(batch_size,8703)))\n",
    "model.add(LSTM(16,return_sequences=True,init='glorot_uniform'))  # try using a GRU instead, for fun\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(input_dim=embedding_dims, nb_filter=num_filters, filter_length=filter_length, border_mode=\"valid\", activation=\"relu\", subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(hidden_dims1))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dense(hidden_dims2))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#model.add(LSTM(16, return_sequences=True)) \n",
    "#model.add(Activation('sigmoid'))\n",
    "#model.add(LSTM(16,return_sequences=True))  # try using a GRU instead, for fun\n",
    "#model.add(Activation('linear'))\n",
    "#model.add(LSTM(16, return_sequences=True)) \n",
    "#model.add(Activation('linear'))\n",
    "#model.add(LSTM(16, return_sequences=True))  \n",
    "#model.add(Activation('sigmoid'))\n",
    "#model.add(GRU(16, input_shape=(batch_size,64),dropout_W=0.2, dropout_U=0.2,return_sequences=True))  # try using a GRU instead, for fun\n",
    "#model.add(Activation('linear'))\n",
    "#model.add(LSTM(16, input_shape=(batch_size,64),dropout_W=0.2, dropout_U=0.2,return_sequences=False))  # try using a GRU instead, for fun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "(4000, 8703)\n",
      "(4000, 2)\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 403s - loss: nan - acc: 0.8628 - val_loss: nan - val_acc: 0.8610\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 439s - loss: nan - acc: 0.8665 - val_loss: nan - val_acc: 0.8610\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 451s - loss: nan - acc: 0.8665 - val_loss: nan - val_acc: 0.8610\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 438s - loss: nan - acc: 0.8665 - val_loss: nan - val_acc: 0.8610\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 447s - loss: nan - acc: 0.8665 - val_loss: nan - val_acc: 0.8610\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 455s - loss: nan - acc: 0.8665 - val_loss: nan - val_acc: 0.8610\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 443s - loss: nan - acc: 0.8665 - val_loss: nan - val_acc: 0.8610\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 435s - loss: nan - acc: 0.8665 - val_loss: nan - val_acc: 0.8610\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 435s - loss: nan - acc: 0.8665 - val_loss: nan - val_acc: 0.8610\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 449s - loss: nan - acc: 0.8665 - val_loss: nan - val_acc: 0.8610\n",
      "1000/1000 [==============================] - 24s    \n",
      "('Test score:', nan)\n",
      "('Test accuracy:', 0.86099999547004702)\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=10,\n",
    "          validation_data=(X_test, Y_test), shuffle=True)\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Save model architecture to an image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(model, to_file='model.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save model parameters to retireve for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)            (100, 8703, 128)    1113984     embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                      (100, 8703, 16)     9280        embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)          (100, 8703, 16)     0           lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)    (100, 8701, 32)     1568        activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_2 (MaxPooling1D)      (100, 4350, 32)     0           convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)          (100, 4350, 32)     0           maxpooling1d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)                (100, 139200)       0           activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                    (100, 250)          34800250    flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)                (100, 250)          0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)          (100, 250)          0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                    (100, 2)            502         activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)                (100, 2)            0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)          (100, 2)            0           dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 35925584\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print model.summary()\n",
    "\n",
    "json_string = model.to_json()\n",
    "open('my_model_architecture.json', 'w').write(json_string)\n",
    "model.save_weights('my_model_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57278\n"
     ]
    }
   ],
   "source": [
    "print np.sum(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 98s    \n"
     ]
    }
   ],
   "source": [
    "prediction_results = model.predict(X_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print np.sum(prediction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
